{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6b18630-2558-4b64-84f8-1ea7a658834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "from tensorflow.keras import layers, Model, Sequential\n",
    "\n",
    "\n",
    "seed_num = 7\n",
    "random.seed(seed_num)\n",
    "np.random.seed(seed_num)\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from evaluation import stratified_concordance_index, stratified_brier_score\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a831b3-500d-427f-9248-62f0b484e119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba6ebc57-9b87-4cf3-a82f-2caefc3e8a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseFeedForwardNet(tf.keras.Model):\n",
    "    \"\"\"Implements the Neural Network Feed Forward Base Class to be used with different Loss Functions.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims=[500, 200, 100, 10]):\n",
    "        \"\"\"\n",
    "        Initializes network architecture.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Dimensionality of input vector\n",
    "            output_dim (int): Dimensionality of output vector\n",
    "        \"\"\"\n",
    "        super(BaseFeedForwardNet, self).__init__()\n",
    "        self.layers_list = []\n",
    "\n",
    "        current_dim = input_dim\n",
    "        for hdim in hidden_dims:\n",
    "            self.layers_list.append(tf.keras.layers.Dense(hdim, activation='relu'))\n",
    "            self.layers_list.append(tf.keras.layers.Dropout(0.4))\n",
    "            current_dim = hdim\n",
    "\n",
    "        self.layers_list.append(tf.keras.layers.Dense(output_dim))\n",
    "\n",
    "    def call(self, x):\n",
    "        output = x\n",
    "        for layer in self.layers_list:\n",
    "            output = layer(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class StratifiedPartialLikelihoodLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Implements the Stratified Partial Likelihood loss function as a custom TensorFlow loss function.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(StratifiedPartialLikelihoodLoss, self).__init__()\n",
    "\n",
    "    def partial_likelihood(self, output, event_time, event_indicator):\n",
    "        \"\"\"\n",
    "        Calculates the negative stratified partial likelihood on neural network output.\n",
    "        \"\"\"\n",
    "        sorted_ind = tf.argsort(event_time)\n",
    "        output = tf.gather(output, sorted_ind)\n",
    "        event_indicator = tf.gather(event_indicator, sorted_ind)\n",
    "        output_uncensored = tf.boolean_mask(output, event_indicator)\n",
    "\n",
    "        accumulated_risk = tf.math.log(\n",
    "            tf.reverse(\n",
    "                tf.math.cumsum(tf.reverse(tf.math.exp(output), axis=[0])), axis=[0]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        uncensored_accumulated_risk = tf.boolean_mask(accumulated_risk, event_indicator)\n",
    "        return -tf.reduce_sum(output_uncensored - uncensored_accumulated_risk)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        output, event_time, event_indicator, strata = y_pred, y_true[:, 0], y_true[:, 1], y_true[:, 2]\n",
    "\n",
    "        if strata is None:\n",
    "            strata = tf.fill(event_indicator.shape, 1)\n",
    "\n",
    "        unique_groups, _ = tf.unique(strata)\n",
    "        p_losses = []\n",
    "\n",
    "        for strat in unique_groups:\n",
    "            indices_strata = tf.where(strata == strat)[:, 0]\n",
    "            p_losses.append(self.partial_likelihood(\n",
    "                tf.gather(output, indices_strata),\n",
    "                tf.gather(event_time, indices_strata),\n",
    "                tf.gather(event_indicator, indices_strata),\n",
    "            ))\n",
    "\n",
    "        return tf.reduce_sum(p_losses)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_keras_feedforward_model(input_dim, output_dim, hidden_dims=[500, 200, 100, 10]):\n",
    "    \"\"\"\n",
    "    Creates a Keras Sequential model equivalent to the BaseFeedForwardNet.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Dimensionality of input vector\n",
    "        output_dim (int): Dimensionality of output vector\n",
    "        hidden_dims (list): List of hidden layer dimensions\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: Keras model equivalent to BaseFeedForwardNet\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(layers.Input(shape=(input_dim,)))\n",
    "\n",
    "    # Hidden layers\n",
    "    for hdim in hidden_dims:\n",
    "        model.add(layers.Dense(hdim, activation='relu'))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(output_dim))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428d1560-f4de-4a69-91b2-b9099bbf72b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "daf77bbd-8c8e-4e0d-95fb-7e4bc1bafc3f",
   "metadata": {},
   "source": [
    "## Load sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "799f5e25-e5c8-4d17-9abc-50ad0421f9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npy_from_url(url):\n",
    "    \"\"\"\n",
    "    Downloads a .npy file from a given URL and loads it as a NumPy array.\n",
    "\n",
    "    Parameters:\n",
    "        url (str): The URL of the .npy file (must be a Raw URL for GitHub files).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The loaded NumPy array.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Download the file from the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "        # Save the content to a temporary file\n",
    "        with open('temp.npy', 'wb') as temp_file:\n",
    "            temp_file.write(response.content)\n",
    "\n",
    "        # Load the NumPy array from the temporary file\n",
    "        data = np.load('temp.npy', allow_pickle=True)\n",
    "        return data\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the file: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the .npy file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9a1f408-3d5a-49bb-adce-00bd84cd041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_X_train = 'https://raw.githubusercontent.com/JunetaeKim/GCSP-HBDA/main/Week11/Data/X_train.npy'\n",
    "url_X_val = 'https://raw.githubusercontent.com/JunetaeKim/GCSP-HBDA/main/Week11/Data/X_val.npy'\n",
    "url_y_train = 'https://raw.githubusercontent.com/JunetaeKim/GCSP-HBDA/main/Week11/Data/y_train.npy'\n",
    "url_y_val = 'https://raw.githubusercontent.com/JunetaeKim/GCSP-HBDA/main/Week11/Data/y_val.npy'\n",
    "\n",
    "X_train = load_npy_from_url(url_X_train)\n",
    "X_val = load_npy_from_url(url_X_val)\n",
    "y_train = load_npy_from_url(url_y_train)\n",
    "y_val = load_npy_from_url(url_y_val)\n",
    "\n",
    "# This is an arbitrary method to facilitate the research setup and is not a standard approach. Please keep this in mind.\n",
    "y_train[:, 0]= y_train[:, 0] * 200\n",
    "y_val[:, 0]= y_val[:, 0] * 200\n",
    "\n",
    "# GitHub Raw URL\n",
    "response = requests.get('https://raw.githubusercontent.com/JunetaeKim/GCSP-HBDA/main/Week11/Data/x_varlist.pkl')\n",
    "x_varlist = pickle.loads(response.content)\n",
    "\n",
    "\n",
    "X_train_DF = pd.DataFrame(X_train, columns=x_varlist)\n",
    "X_val_DF = pd.DataFrame(X_val, columns=x_varlist)\n",
    "Y_train_DF = pd.DataFrame(y_train, columns=['time','event', 'stratum'])\n",
    "Y_val_DF = pd.DataFrame(y_val, columns=['time','event', 'stratum'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a76901-be73-4555-9407-4cae958976a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ae8593f-9b5a-44c2-80be-42dbb0c7b6f8",
   "metadata": {},
   "source": [
    "### Parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d599b8e4-7e7a-49f4-ba75-5a85a29e4621",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "output_dim = 1\n",
    "hidden_dims = [500, 200, 100, 50]\n",
    "batch_size = 500\n",
    "epochs = 200\n",
    "weight_save_path = './Save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85246c35-d194-492c-a68d-bb3dab880610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65d20c8f-2246-4ac3-b9f5-2934e3f09513",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2dd62ba-77f5-4ca2-b3db-431e10447c42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at epoch of  0\n",
      "Epoch 1, Loss: 79.87882995605469, val_loss: 956.329345703125\n",
      "Model saved at epoch of  1\n",
      "Epoch 2, Loss: 65.66600036621094, val_loss: 954.9307250976562\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m model(X_batch, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_fn(y_batch, y_pred)\n\u001b[1;32m---> 22\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m \u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(gradients, model\u001b[38;5;241m.\u001b[39mtrainable_variables))\n\u001b[0;32m     25\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m loss_fn(y_val, model(X_val, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:1113\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1107\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1108\u001b[0m       composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[0;32m   1109\u001b[0m           output_gradients))\n\u001b[0;32m   1110\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[0;32m   1111\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[1;32m-> 1113\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m \u001b[43mimperative_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[0;32m   1122\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[0;32m   1123\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:160\u001b[0m, in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    158\u001b[0m     gradient_name_scope \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m forward_pass_name_scope \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmock_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mout_grads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m grad_fn(mock_op, \u001b[38;5;241m*\u001b[39mout_grads)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:674\u001b[0m, in \u001b[0;36m_ExpGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcontrol_dependencies([grad]):\n\u001b[0;32m    673\u001b[0m   y \u001b[38;5;241m=\u001b[39m math_ops\u001b[38;5;241m.\u001b[39mconj(y)\n\u001b[1;32m--> 674\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgrad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1407\u001b[0m, in \u001b[0;36m_OverrideBinaryOperatorHelper.<locals>.binary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1403\u001b[0m   \u001b[38;5;66;03m# force_same_dtype=False to preserve existing TF behavior\u001b[39;00m\n\u001b[0;32m   1404\u001b[0m   \u001b[38;5;66;03m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[39;00m\n\u001b[0;32m   1405\u001b[0m   \u001b[38;5;66;03m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[39;00m\n\u001b[0;32m   1406\u001b[0m   x, y \u001b[38;5;241m=\u001b[39m maybe_promote_tensors(x, y)\n\u001b[1;32m-> 1407\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1408\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1409\u001b[0m   \u001b[38;5;66;03m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m   \u001b[38;5;66;03m# object that can implement the operator with knowledge of itself\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1413\u001b[0m   \u001b[38;5;66;03m# original error from the LHS, because it may be more\u001b[39;00m\n\u001b[0;32m   1414\u001b[0m   \u001b[38;5;66;03m# informative.\u001b[39;00m\n\u001b[0;32m   1415\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(y), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__r\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m op_name):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1767\u001b[0m, in \u001b[0;36m_mul_dispatch\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1765\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m sparse_tensor\u001b[38;5;241m.\u001b[39mSparseTensor(y\u001b[38;5;241m.\u001b[39mindices, new_vals, y\u001b[38;5;241m.\u001b[39mdense_shape)\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1767\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:529\u001b[0m, in \u001b[0;36mmultiply\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmath.multiply\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    481\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39mregister_binary_elementwise_api\n\u001b[0;32m    482\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmultiply\u001b[39m(x, y, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    484\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns an element-wise x * y.\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \n\u001b[0;32m    486\u001b[0m \u001b[38;5;124;03m  For example:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[38;5;124;03m   * InvalidArgumentError: When `x` and `y` have incompatible shapes or types.\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 529\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_math_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:6575\u001b[0m, in \u001b[0;36mmul\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   6573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   6574\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 6575\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   6576\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMul\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   6578\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define model, loss and optimizer\n",
    "model = BaseFeedForwardNet(input_dim, output_dim, hidden_dims)\n",
    "loss_fn = StratifiedPartialLikelihoodLoss()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "best_score = np.inf\n",
    "num_batches = int(np.ceil(len(X_train) / batch_size))  \n",
    "for epoch in range(epochs):\n",
    "    # Shuffle the data at the start of each epoch\n",
    "    permutation = np.random.permutation(len(X_train))\n",
    "    X_train = X_train[permutation]\n",
    "    y_train = y_train[permutation]\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        X_batch = X_train[i * batch_size: (i + 1) * batch_size]\n",
    "        y_batch = y_train[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch, training=True)\n",
    "            loss = loss_fn(y_batch, y_pred)\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    val_loss = loss_fn(y_val, model(X_val, training=False))\n",
    "\n",
    "    if val_loss <= best_score:\n",
    "        best_score = val_loss\n",
    "        model.save_weights(weight_save_path + '/StratfiedNN.h5')  # Save the model weights\n",
    "        print('Model saved at epoch of ', epoch)\n",
    "        \n",
    "\n",
    "    # Print loss for each epoch\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.numpy()}, val_loss: {val_loss.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4204b752-ad01-42da-a986-713ccc80fe6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c93e9b3-dc50-4543-b0af-e8132bfb6131",
   "metadata": {},
   "source": [
    "### Load trained weights from the local disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57bf3e47-043f-46ff-afb9-5a9e4c6f62e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(weight_save_path + '/StratfiedNN.h5')\n",
    "keras_model = create_keras_feedforward_model(input_dim, output_dim, hidden_dims)\n",
    "keras_model.set_weights([w.numpy() for w in model.trainable_weights])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7397c9-9060-477c-9815-f83351b06755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf2dda0a-cab1-451d-90b6-d10eb057c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRIER_EVAL_TIME = int(min(np.max(y_train[:, 0]), np.max(y_val[:, 0]))) # End of the investigated time period for which the brier score is computed \n",
    "\n",
    "\n",
    "# Structured arrays for Brier Score Evaluation.\n",
    "survival_data_train = np.zeros(y_train.shape[0],\n",
    "    dtype={'names':('event_indicator', 'event_time'), 'formats':('bool', 'u2')})\n",
    "survival_data_train['event_indicator'] = y_train[:, 1]\n",
    "survival_data_train['event_time'] = y_train[:, 0]\n",
    "\n",
    "survival_data_test = np.zeros(y_val.shape[0],\n",
    "    dtype={'names':('event_indicator', 'event_time'), 'formats':('bool', 'u2')})\n",
    "survival_data_test['event_indicator'] = y_val[:, 1]\n",
    "survival_data_test['event_time'] = y_val[:, 0]\n",
    "\n",
    "strata_train = y_train[:, 2]\n",
    "strata_test = y_val[:, 2]\n",
    "\n",
    "\n",
    "numpy_output_train = tf.squeeze(model(X_train), axis=1).numpy()\n",
    "numpy_output_test = tf.squeeze(model(X_val), axis=1).numpy()\n",
    "\n",
    "\n",
    "brier_score = stratified_brier_score(\n",
    "    BRIER_EVAL_TIME,\n",
    "    survival_data_train,\n",
    "    survival_data_test,\n",
    "    numpy_output_train,\n",
    "    numpy_output_test,\n",
    "    strata_train=strata_train,\n",
    "    strata_test=strata_test\n",
    ")\n",
    "\n",
    "\n",
    "c_index = stratified_concordance_index(\n",
    "    numpy_output_test,\n",
    "    survival_data_test['event_indicator'],\n",
    "    survival_data_test['event_time'],\n",
    "    strata_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8bd242-5aa3-40bd-bc04-f780b80ace7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
